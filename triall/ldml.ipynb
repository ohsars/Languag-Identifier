{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../newThings/newDataset.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                     Text Language\n",
       "0       And I say: If you don't know you're a slave, ...  English\n",
       "1                                       And she's right.  English\n",
       "2       Because it's scary and it's expensive, but we...  English\n",
       "3          I want them to think, Well there's some hope.  English\n",
       "4                        It took only three generations.  English\n",
       "...                                                  ...      ...\n",
       "33827                                             pencil  English\n",
       "33828                                               shit  English\n",
       "33829  Sharpener, Eraser, stationeries are stationary...  English\n",
       "33830  Who fries, fried, jammed, jam and stock, rathe...  English\n",
       "33831          flattery happens all the time here, yunno  English\n",
       "\n",
       "[33832 rows x 2 columns]>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape\n",
    "dataset.head"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  427,   460,   701,   703,  1030,  1201,  1955,  1959,  2349,\n",
      "        2414,  2466,  2574,  2794,  3503,  3522,  3736,  3746,  4326,\n",
      "        4655,  4858,  4983,  4988,  5028,  5054,  5145,  6761,  6791,\n",
      "        7581,  7685,  7702,  7800,  8065,  8515,  8601,  9286,  9440,\n",
      "        9567,  9578,  9636,  9661,  9697,  9803,  9846,  9874, 10014,\n",
      "       10045, 10311, 10314, 10968, 11083, 11957, 12017, 14308, 15311,\n",
      "       15333, 15417, 15439, 15779, 15808, 15928, 16431, 16529, 16561,\n",
      "       16887, 16949, 16980, 17013, 17053, 17559, 18099, 18298, 18382,\n",
      "       18673, 18687, 19019, 19272, 19298, 19605, 19681, 19757],\n",
      "      dtype=int64), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# get the boolean mask of null values\n",
    "mask = dataset.isnull()\n",
    "\n",
    "# get the indices of the True values in the boolean mask\n",
    "indices = np.where(mask)\n",
    "\n",
    "# print the indices\n",
    "print(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Horsars Marvel\\AppData\\Local\\Temp\\ipykernel_16184\\3923587421.py:4: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  dataset.fillna(dataset.mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "dataset.isnull().sum()\n",
    "\n",
    "# Replace NaN values with the mean of the column\n",
    "dataset.fillna(dataset.mean(), inplace=True)\n",
    "\n",
    "# Remove rows with NaN values\n",
    "dataset.dropna(inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Languages in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text        0\n",
       "Language    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "English       13166\n",
       "Yoruba        12539\n",
       "French         1014\n",
       "Spanish         819\n",
       "Portugeese      739\n",
       "Italian         698\n",
       "Russian         692\n",
       "Sweedish        676\n",
       "Malayalam       594\n",
       "Dutch           546\n",
       "Turkish         474\n",
       "German          470\n",
       "Tamil           469\n",
       "Danish          428\n",
       "Greek           365\n",
       "Hindi            63\n",
       "Name: Language, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"Language\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split input and then Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(dataset[\"Text\"])\n",
    "y = np.array(dataset[\"Language\"])\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(x)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer is a great tool provided by the Scikik-learn Library in Python. It is used to transform a given text into a ventor on the basis of the frequency (count) of each word that occurs in the entire text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9701948110243289"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Danish: 0.00%\n",
      "Dutch: 0.00%\n",
      "English: 0.00%\n",
      "French: 0.00%\n",
      "German: 0.00%\n",
      "Greek: 0.00%\n",
      "Hindi: 0.00%\n",
      "Italian: 0.00%\n",
      "Malayalam: 0.00%\n",
      "Portugeese: 0.00%\n",
      "Russian: 0.00%\n",
      "Spanish: 0.00%\n",
      "Sweedish: 0.00%\n",
      "Tamil: 0.00%\n",
      "Turkish: 0.00%\n",
      "Yoruba: 100.00%\n"
     ]
    }
   ],
   "source": [
    "for i, lang in enumerate(model.classes_):\n",
    "    print(f\"{lang}: {probas[0][i]:.2%}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is a problem of multiclass classification, so we will be using the Multinomial Naive Bayes algorithm to train the language detection model. This algorithm always perform very well on the problem based on multiclass classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sampleTexts = 'going to lagos', 'ooni', 'fish', 'pin', 'sharpener', 'one', 'mono', 'monospace', '', '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ooni\n",
      "['English']\n"
     ]
    }
   ],
   "source": [
    "# user = input(\"Enter a Text: \")\n",
    "user = \"ooni\"\n",
    "dataset = cv.transform([user]).toarray()\n",
    "output = model.predict(dataset)\n",
    "\n",
    "print(user)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
